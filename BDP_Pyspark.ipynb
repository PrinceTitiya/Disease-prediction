{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyxlsb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhWBKD3qmp1p",
        "outputId": "cadd4ec7-f1b2-4067-cbca-3e0eab6a28dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyxlsb\n",
            "  Downloading pyxlsb-1.0.10-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyxlsb\n",
            "Successfully installed pyxlsb-1.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYfAOtRkn2gr",
        "outputId": "661c94d3-ebfa-49ac-d674-291fc61b2cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=f9b18177920ba89699ad3be493d41b1b104a91f0b0bfad227cee6354f1e1d486\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier, LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Step 1: Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DiseasePrediction\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Load data\n",
        "data_old = pd.read_excel(\"/content/generated_data.xlsb\")\n",
        "data = spark.createDataFrame(data_old)\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "categorical_columns_data = [\"Gender\", \"Fever\", \"Cough\", \"Headache\", \"Nausea\", \"Vomiting\", \"pain chest\",\n",
        "                       \"shortness of breath\", \"dizziness\", \"asthenia\", \"fall\", \"syncope\", \"vertigo\",\n",
        "                       \"sweat\", \"sweating increased\",\"palpitation\",\"angina pectoris\",\"pressure chest\",\n",
        "                       \"polyuria\",\"polydypsia\",\"orthopnea\",\"rale\",\"unresponsiveness\",\"hallucinations visual\",\n",
        "                       \"bedridden\",\"prostatism\"]\n",
        "\n",
        "# Target variable encoding\n",
        "labelindex = StringIndexer(inputCol=\"Disease\", outputCol=\"Disease_index\")\n",
        "data = labelindex.fit(data).transform(data)\n",
        "\n",
        "# Categorical variable encoding\n",
        "indexers_to_encode = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\")\n",
        "            for column in categorical_columns_data]\n",
        "\n",
        "# Fit and transform the DataFrame with StringIndexer\n",
        "for indexer in indexers_to_encode:\n",
        "    data = indexer.fit(data).transform(data)\n",
        "\n",
        "# Step 4: Assemble features\n",
        "featurecolumns = [col + \"_index\" for col in categorical_columns_data]\n",
        "assembler = VectorAssembler(inputCols=featurecolumns, outputCol=\"features\")\n",
        "preprocessed_data = assembler.transform(data)\n",
        "\n",
        "# Step 5: Train-Test Split (optional)\n",
        "# In PySpark, train-test split is usually not required as it handles the entire dataset\n",
        "\n",
        "# Step 6: Model Training and Evaluation\n",
        "diff_models_used = [\n",
        "    RandomForestClassifier(featuresCol=\"features\", labelCol=\"Disease_index\", numTrees=100, seed=42),\n",
        "    DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Disease_index\", seed=42),\n",
        "    LogisticRegression(featuresCol=\"features\", labelCol=\"Disease_index\")]\n",
        "\n",
        "for classifier in diff_models_used:\n",
        "    # Train the model\n",
        "    model = classifier.fit(preprocessed_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions_made = model.transform(preprocessed_data)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluator_for_model = MulticlassClassificationEvaluator(labelCol=\"Disease_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator_for_model.evaluate(predictions_made)\n",
        "    print(\"Model Name:\", type(classifier).__name__)\n",
        "    print(\"Accuracy Achieved:\", accuracy)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu5oz59w0fWd",
        "outputId": "821aa182-6e94-4e16-c990-e6134f7aa718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Name: RandomForestClassifier\n",
            "Accuracy Achieved: 0.2676985353970708\n",
            "Model Name: DecisionTreeClassifier\n",
            "Accuracy Achieved: 0.25721651443302884\n",
            "Model Name: LogisticRegression\n",
            "Accuracy Achieved: 0.2533785067570135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier, OneVsRest\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"Disease_index\", maxIter=10)\n",
        "\n",
        "new_model = OneVsRest(classifier=gbt, labelCol=\"Disease_index\")\n",
        "\n",
        "ovrmodel = new_model.fit(preprocessed_data)\n",
        "\n",
        "predictions = ovrmodel.transform(preprocessed_data)\n",
        "\n",
        "evaluator_for_model = MulticlassClassificationEvaluator(labelCol=\"Disease_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator_for_model.evaluate(predictions)\n",
        "print(\"Accuracy Achieved:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5o51WBGGxd5",
        "outputId": "9e9e8467-cc05-430e-fe91-01b2856266d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Achieved: 0.26829053658107316\n"
          ]
        }
      ]
    }
  ]
}